{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        " Q.1.What is Simple Linear Regression?\n",
        " Ans:Linear regression is a machine learning algorithm that uses a linear equation to predict the value of a dependent variable based on a\n",
        " known independent variable."
      ],
      "metadata": {
        "id": "CXJ0PELT-fGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.2.What are the key assumptions of Simple Linear Regression?\n",
        "Ans:The key assumptions of Simple Linear Regression are: linearity (a straight-line relationship between independent and dependent variables),\n",
        "independence of errors (residuals are not correlated with each other), homoscedasticity (constant variance of errors across the range of the\n",
        "independent variable), normality of residuals (errors are normally distributed), and no multicollinearity (independent variables are not highly\n",
        "correlated with each other)."
      ],
      "metadata": {
        "id": "P2GJAJqd-g3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.3.What does the coefficient m represent in the equation Y=mX+c?\n",
        "Ans:The equation y = mx + c is the general equation of any straight line where m is the gradient of the line (how steep the line is) and c is the\n",
        "y -intercept (the point in which the line crosses the y -axis)."
      ],
      "metadata": {
        "id": "iU-XPlSV_7Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.4.What does the intercept c represent in the equation Y=mX+c?\n",
        "Ans:\"c\" in the equation y = mx + c, stands for, Y intercept. (0, c) is the point at which the line y = mx + c intersects the y-axis."
      ],
      "metadata": {
        "id": "_Yn1cC5HAZ80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.5.How do we calculate the slope m in Simple Linear Regression?\n",
        "Ans:To calculate slope for a regression line, you'll need to divide the standard deviation of y values by the standard deviation of x values and\n",
        "then multiply this by the correlation between x and y. The slope can be negative, which would show a line going downhill rather than upwards."
      ],
      "metadata": {
        "id": "L3npbHKDA4Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.6.What is the purpose of the least squares method in Simple Linear Regression?\n",
        "Ans:In Simple Linear Regression, the least squares method is used to find the \"best fit\" line for a set of data points by minimizing the sum of\n",
        "the squared differences between the actual data points and the predicted values on that line, essentially aiming to create a line that comes as\n",
        "close as possible to all data points in the dataset, allowing for the most accurate prediction of dependent variable values based on the\n",
        "independent variable"
      ],
      "metadata": {
        "id": "LpA5A1yjC9qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "Ans:You can interpret the coefficient of determination (R²) as the proportion of variance in the dependent variable that is predicted by the\n",
        "statistical model. Another way of thinking of it is that the R² is the proportion of variance that is shared between the independent and dependent\n",
        "variables."
      ],
      "metadata": {
        "id": "q-CY4ROLEUYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.8. What is Multiple Linear Regression?\n",
        "Ans:Multiple linear regression (MLR) is a statistical technique that uses multiple independent variables to predict the value of a dependent\n",
        "variable. It's used to analyze the relationship between variables when there are more than one independent variable."
      ],
      "metadata": {
        "id": "bWr9hia-q7DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.9.What is the main difference between Simple and Multiple Linear Regression?\n",
        "Ans:The primary difference between simple linear regression and multiple linear regression is the number of independent variables used to predict\n",
        "a dependent variable: simple linear regression uses only one independent variable, while multiple linear regression uses two or more independent\n",
        "variables."
      ],
      "metadata": {
        "id": "hkHlk6S7rP4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.10.What are the key assumptions of Multiple Linear Regression?\n",
        "Ans:The key assumptions of Multiple Linear Regression are: linearity (a straight-line relationship between the independent and dependent variables),\n",
        "independence of errors (residuals are not correlated with each other), homoscedasticity (constant variance of errors across all levels of the\n",
        "independent variables), no multicollinearity (independent variables are not highly correlated with each other), and normality of residuals\n",
        "(the distribution of errors is approximately normal)."
      ],
      "metadata": {
        "id": "B_e8JWIHrrvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.11.What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "Ans:Heteroscedasticity refers to a situation where the variance of the error terms (residuals) in a regression model is not constant across\n",
        "different values of the independent variables."
      ],
      "metadata": {
        "id": "0gJjN-f1sNHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "Ans: remove highly correlated predictor variables, use feature engineering techniques like creating new variables combining correlated ones,\n",
        "apply regularization methods like Ridge or Lasso regression, or consider collecting more data with a wider range of values for the correlated\n",
        "variables if possible"
      ],
      "metadata": {
        "id": "UA7sz-LXuTnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.13.What are some common techniques for transforming categorical variables for use in regression models?\n",
        "Ans:one-hot encoding (creating binary vectors for each category), label encoding (assigning numerical labels to categories), ordinal encoding\n",
        "(for categorical variables with a natural order), and dummy variable creation (generating new binary variables for each category), with the choice\n",
        "depending on whether the categories have an inherent order or not."
      ],
      "metadata": {
        "id": "o5p0pZcCwUgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.14.What is the role of interaction terms in Multiple Linear Regression?\n",
        "Ans:In multiple linear regression, interaction terms are used to capture how the effect of one independent variable on the dependent variable\n",
        "changes depending on the value of another independent variable, essentially allowing the model to account for situations where the combined\n",
        "effect of two variables is not simply additive but interactive, meaning the relationship between one variable and the outcome varies based on\n",
        "the level of the other variable"
      ],
      "metadata": {
        "id": "zMNhuS7lxhBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "Ans:In a simple linear regression, the intercept represents the predicted value of the dependent variable when the independent variable is equal\n",
        "to zero, while in a multiple linear regression, the intercept represents the predicted value of the dependent variable when all independent\n",
        "variables are set to zero; essentially, it's the \"baseline\" value when none of the predictor variables have any effect."
      ],
      "metadata": {
        "id": "Dc9DgvldxhHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "Ans:The slope of regression in species-area relationship predicts species richness of an area. It indicates the dependency of species richness\n",
        "on the area as higher slope reflects higher dependency of the area. Taking into account of a large area, such as country, the slope is almost\n",
        "linear with the area."
      ],
      "metadata": {
        "id": "Av0ucygUxhK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.17.How does the intercept in a regression model provide context for the relationship between variables?\n",
        "Ans:In a regression model, the intercept represents the predicted value of the dependent variable when all independent variables are set to zero,\n",
        "essentially providing a baseline understanding of the relationship between variables by indicating where the regression line crosses the y-axis,\n",
        "thus giving context to the overall trend by showing the starting point of the relationship when no change in the independent variable occurs."
      ],
      "metadata": {
        "id": "Pg8P8mQixhOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.18.What are the limitations of using R² as a sole measure of model performance?\n",
        "Ans:However, it doesn't tell you whether your chosen model is good or bad, nor will it tell you whether the data and predictions are biased.\n",
        "A high or low R-squared isn't necessarily good or bad—it doesn't convey the reliability of the model or whether you've chosen the right regression."
      ],
      "metadata": {
        "id": "zVNnKvJ0xhRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.19. How would you interpret a large standard error for a regression coefficient?\n",
        "Ans:A large standard error for a regression coefficient indicates that the estimated coefficient is less reliable and has a higher degree of\n",
        "variability, meaning there is a greater chance that the true population value could be significantly different from the sample estimate based on\n",
        "the data collected; essentially, it suggests that the coefficient is not very precise and could vary considerably if you were to repeat the analysis\n",
        "with a different sample from the same population."
      ],
      "metadata": {
        "id": "Sf0oMNSCxhUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "Ans:Heteroscedasticity in a residual plot is typically identified by a \"fan\" or \"cone\" shape, where the spread of the residuals increases as the\n",
        "fitted values increase, indicating that the variance of the errors is not constant across the range of predicted values; this is considered\n",
        "problematic because it can lead to unreliable hypothesis tests and standard errors in a regression model, making it crucial to address."
      ],
      "metadata": {
        "id": "-8SXx1PlxhXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.21.What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "Ans:If a Multiple Linear Regression model has a high R² but a low adjusted R², it indicates that the model is likely overfitting the data,\n",
        "meaning it is explaining a large portion of the variability in the dependent variable but is doing so by capturing noise in the data rather than\n",
        "the true underlying relationships, often due to including too many independent variables that may not be significantly contributing to the model."
      ],
      "metadata": {
        "id": "1IekN-9yxhax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.22.Why is it important to scale variables in Multiple Linear Regression?\n",
        "Ans:Scaling variables in Multiple Linear Regression is important because it ensures that all independent variables contribute equally to the model,\n",
        "making the coefficients more interpretable and improving the overall performance of the model, especially when variables have vastly different\n",
        "scales or units of measurement; this is particularly relevant when using optimization algorithms like gradient descent where scaling can lead to\n",
        "faster convergence."
      ],
      "metadata": {
        "id": "V-6ld1T4xhd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.23.What is polynomial regression?\n",
        "Ans:Polynomial regression is a machine learning technique that models non-linear relationships between variables. It's an extension of linear\n",
        "regression, which assumes a linear relationship between variables."
      ],
      "metadata": {
        "id": "xU_K916Nxhhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.24.How does polynomial regression differ from linear regression?\n",
        "Ans:polynomial regression allows for more complex, non-linear relationships by incorporating higher-order powers of the independent variable,\n",
        "essentially creating curves to fit the data better when a linear relationship isn't present; making it more flexible for capturing complex\n",
        "patterns in data where the relationship between variables isn't a straight line."
      ],
      "metadata": {
        "id": "oGo_QDtExhk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.25.When is polynomial regression used?\n",
        "Ans:Polynomial regression is a machine learning algorithm used to model non-linear relationships between variables. It's used when there's\n",
        "no linear correlation between the variables, or when a straight line can't accurately predict the outcomes."
      ],
      "metadata": {
        "id": "E2o_V0ybxhpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.26.What is the general equation for polynomial regression?\n",
        "Ans:We have just implemented polynomial regression - as easy as that! In general, polynomial models are of the\n",
        "form y=f(x)=β0+β1x+β2x2+β3x3+… +βdxd+ + β d x d + ϵ , where d is called the degree of the polynomial."
      ],
      "metadata": {
        "id": "WCqmIJ-gxhtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.27.Can polynomial regression be applied to multiple variables?\n",
        "Ans:Yes, polynomial regression can be applied to multiple variables, which is often referred to as \"multivariate polynomial regression\";\n",
        "it allows you to model non-linear relationships between a dependent variable and several independent variables by raising each independent\n",
        "variable to different powers, essentially creating a polynomial equation with multiple input variables."
      ],
      "metadata": {
        "id": "S8T-qr1dxh0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.28.What are the limitations of polynomial regression?\n",
        "Ans:Overfitting: Higher-degree polynomial models are susceptible to overfitting, where the model fits the training data too closely and loses\n",
        "generalization ability. Careful model selection and regularization techniques are required to mitigate this risk."
      ],
      "metadata": {
        "id": "ZAMLVxPRxh3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.29.What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "Ans:visual inspection of the data and fitted curve, using a validation set to assess performance with different degrees, cross-validation to\n",
        "prevent overfitting, and statistical metrics like R-squared and Mean Squared Error (MSE); where a good fit balances capturing the data's trend\n",
        "without excessive complexity (overfitting) by choosing the appropriate polynomial degree."
      ],
      "metadata": {
        "id": "t3qlrZyUxh6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.30.Why is visualization important in polynomial regression?\n",
        "Ans:Visualization is crucial in polynomial regression because it allows you to visually assess how well the fitted polynomial curve captures the\n",
        "underlying non-linear relationship in your data, helping you identify potential issues like overfitting or underfitting, and ultimately determine\n",
        "if the model is accurately representing the complex patterns within your dataset."
      ],
      "metadata": {
        "id": "eTL7GK9-xjc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.31.How is polynomial regression implemented in Python?\n",
        "Ans:Implement Polynomial Regression in Python\n",
        "The polynomial regression model is then trained by adjusting the coefficients of the polynomial terms to minimize the difference between the\n",
        "observed and predicted values of the dependent variable. The resulting equation can then be used to make predictions for new data."
      ],
      "metadata": {
        "id": "6EZ64eQUxjj3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}